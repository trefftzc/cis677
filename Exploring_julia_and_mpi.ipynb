{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNwn0BSbXcCu9h9FzP1ysze",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trefftzc/cis677/blob/main/Exploring_julia_and_mpi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using MPI in Julia"
      ],
      "metadata": {
        "id": "7IDoSJXiMoOq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!julia -e 'using Pkg; pkg\"add MPI; precompile;\"' &> /dev/null"
      ],
      "metadata": {
        "id": "oHnqzVjX8YvU"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!julia -e 'using Pkg; pkg\"add Statistics; precompile;\"' &> /dev/null"
      ],
      "metadata": {
        "id": "c-ye4wqr9fe9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A first program with Julia and MPI\n",
        "\n",
        "The following examples are taken from this web site:\n",
        "\n",
        "https://juliaparallel.org/MPI.jl/stable/examples/"
      ],
      "metadata": {
        "id": "SbaOqF37OKMo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile 01-hello.jl\n",
        "using MPI\n",
        "MPI.Init()\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "println(\"Hello world, I am $(MPI.Comm_rank(comm)) of $(MPI.Comm_size(comm))\")\n",
        "MPI.Barrier(comm)\n",
        "MPI.Finalize()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0O7Er7QAOM8I",
        "outputId": "db551486-db3c-4bc3-f3c7-f35413da64ba"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing 01-hello.jl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQo-ONx8kfle",
        "outputId": "c0f7b991-0edd-4a41-afa3-e8118d958088"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "01-hello.jl  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install mpiexecjl to avoid problems with the version of MPI"
      ],
      "metadata": {
        "id": "u8DdN2yUQwsQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!julia -e 'using MPI; MPI.install_mpiexecjl()'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgeS76YiPsdQ",
        "outputId": "9b4c8d26-4625-4b12-88e4-8c465e4bd970"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mInstalling `mpiexecjl` to `/root/.julia/bin`...\n",
            "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mDone!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "/root/.julia/bin/mpiexecjl --project=/content -n 3 julia 01-hello.jl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBqLmjOaOUrA",
        "outputId": "b853a27e-8e39-4f45-863b-e49901dbbf2a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello world, I am 0 of 3\n",
            "Hello world, I am 2 of 3\n",
            "Hello world, I am 1 of 3\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# An example of Broadcast"
      ],
      "metadata": {
        "id": "6vMEI5TuToTB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile 02-broadcast.jl\n",
        "import MPI\n",
        "MPI.Init()\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "N = 5\n",
        "root = 0\n",
        "\n",
        "if MPI.Comm_rank(comm) == root\n",
        "    print(\" Running on $(MPI.Comm_size(comm)) processes\\n\")\n",
        "end\n",
        "MPI.Barrier(comm)\n",
        "\n",
        "if MPI.Comm_rank(comm) == root\n",
        "    A = [i*(1.0 + im*2.0) for i = 1:N]\n",
        "else\n",
        "    A = Array{ComplexF64}(undef, N)\n",
        "end\n",
        "\n",
        "MPI.Bcast!(A, root, comm)\n",
        "\n",
        "print(\"rank = $(MPI.Comm_rank(comm)), A = $A\\n\")\n",
        "\n",
        "if MPI.Comm_rank(comm) == root\n",
        "    B = Dict(\"foo\" => \"bar\")\n",
        "else\n",
        "    B = nothing\n",
        "end\n",
        "\n",
        "B = MPI.bcast(B, root, comm)\n",
        "print(\"rank = $(MPI.Comm_rank(comm)), B = $B\\n\")\n",
        "\n",
        "if MPI.Comm_rank(comm) == root\n",
        "    f = x -> x^2 + 2x - 1\n",
        "else\n",
        "    f = nothing\n",
        "end\n",
        "\n",
        "f = MPI.bcast(f, root, comm)\n",
        "print(\"rank = $(MPI.Comm_rank(comm)), f(3) = $(f(3))\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWJvZmEIQV2Q",
        "outputId": "215bc2e6-1b9c-4efa-ad4a-1ad80c4e4218"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing 02-broadcast.jl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now executing the code"
      ],
      "metadata": {
        "id": "JwCEcZ07TwwX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!/root/.julia/bin/mpiexecjl --project=/content -n 4 julia 02-broadcast.jl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDSuft2oTys3",
        "outputId": "9fbd6404-bcee-4aa7-971f-4bda3db40003"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Running on 4 processes\n",
            "rank = 0, A = ComplexF64[1.0 + 2.0im, 2.0 + 4.0im, 3.0 + 6.0im, 4.0 + 8.0im, 5.0 + 10.0im]\n",
            "rank = 3, A = ComplexF64[1.0 + 2.0im, 2.0 + 4.0im, 3.0 + 6.0im, 4.0 + 8.0im, 5.0 + 10.0im]\n",
            "rank = 2, A = ComplexF64[1.0 + 2.0im, 2.0 + 4.0im, 3.0 + 6.0im, 4.0 + 8.0im, 5.0 + 10.0im]\n",
            "rank = 1, A = ComplexF64[1.0 + 2.0im, 2.0 + 4.0im, 3.0 + 6.0im, 4.0 + 8.0im, 5.0 + 10.0im]\n",
            "rank = 0, B = Dict(\"foo\" => \"bar\")\n",
            "rank = 2, B = Dict(\"foo\" => \"bar\")\n",
            "rank = 3, B = Dict(\"foo\" => \"bar\")\n",
            "rank = 1, B = Dict(\"foo\" => \"bar\")\n",
            "rank = 0, f(3) = 14\n",
            "rank = 3, f(3) = 14\n",
            "rank = 2, f(3) = 14\n",
            "rank = 1, f(3) = 14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now an example of reduce"
      ],
      "metadata": {
        "id": "6uw5NN1uUHof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile 03-reduce.jl\n",
        "# This example shows how to use custom datatypes and reduction operators\n",
        "# It computes the variance in parallel in a numerically stable way\n",
        "\n",
        "using MPI, Statistics\n",
        "\n",
        "MPI.Init()\n",
        "const comm = MPI.COMM_WORLD\n",
        "const root = 0\n",
        "\n",
        "# Define a custom struct\n",
        "# This contains the summary statistics (mean, variance, length) of a vector\n",
        "struct SummaryStat\n",
        "    mean::Float64\n",
        "    var::Float64\n",
        "    n::Float64\n",
        "end\n",
        "function SummaryStat(X::AbstractArray)\n",
        "    m = mean(X)\n",
        "    v = varm(X,m, corrected=false)\n",
        "    n = length(X)\n",
        "    SummaryStat(m,v,n)\n",
        "end\n",
        "\n",
        "# Define a custom reduction operator\n",
        "# this computes the pooled mean, pooled variance and total length\n",
        "function pool(S1::SummaryStat, S2::SummaryStat)\n",
        "    n = S1.n + S2.n\n",
        "    m = (S1.mean*S1.n + S2.mean*S2.n) / n\n",
        "    v = (S1.n * (S1.var + S1.mean * (S1.mean-m)) +\n",
        "         S2.n * (S2.var + S2.mean * (S2.mean-m)))/n\n",
        "    SummaryStat(m,v,n)\n",
        "end\n",
        "\n",
        "# Register the custom reduction operator.  This is necessary only on platforms\n",
        "# where Julia doesn't support closures as cfunctions (e.g. ARM), but can be used\n",
        "# on all platforms for consistency.\n",
        "MPI.@RegisterOp(pool, SummaryStat)\n",
        "\n",
        "X = randn(10,3) .* [1,3,7]'\n",
        "\n",
        "# Perform a scalar reduction\n",
        "summ = MPI.Reduce(SummaryStat(X), pool, comm; root)\n",
        "\n",
        "if MPI.Comm_rank(comm) == root\n",
        "    @show summ.var\n",
        "end\n",
        "\n",
        "# Perform a vector reduction:\n",
        "# the reduction operator is applied elementwise\n",
        "col_summ = MPI.Reduce(mapslices(SummaryStat,X,dims=1), pool, comm; root)\n",
        "\n",
        "if MPI.Comm_rank(comm) == root\n",
        "    col_var = map(summ -> summ.var, col_summ)\n",
        "    @show col_var\n",
        "end"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVQro-N0UGLw",
        "outputId": "ac9703b0-6ec5-41a4-94db-9579d4c6af47"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing 03-reduce.jl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now execute the reduction example"
      ],
      "metadata": {
        "id": "7XCBGrWQUrU4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!/root/.julia/bin/mpiexecjl --project=/content -n 4 julia 03-reduce.jl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UbnkgU7Ug1g",
        "outputId": "ef4a9646-2c12-42cb-daa1-12e6b15fa158"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "summ.var = 21.87542746561436\n",
            "col_var = [0.8366775912196903 7.70442408073846 57.071720035674375]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "An example of send and receive"
      ],
      "metadata": {
        "id": "AvOBq4VUUx63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile 04-sendrecv.jl\n",
        "using MPI\n",
        "MPI.Init()\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = MPI.Comm_rank(comm)\n",
        "size = MPI.Comm_size(comm)\n",
        "\n",
        "dst = mod(rank+1, size)\n",
        "src = mod(rank-1, size)\n",
        "\n",
        "N = 4\n",
        "\n",
        "send_mesg = Array{Float64}(undef, N)\n",
        "recv_mesg = Array{Float64}(undef, N)\n",
        "\n",
        "fill!(send_mesg, Float64(rank))\n",
        "\n",
        "rreq = MPI.Irecv!(recv_mesg, comm; source=src, tag=src+32)\n",
        "\n",
        "print(\"$rank: Sending   $rank -> $dst = $send_mesg\\n\")\n",
        "sreq = MPI.Isend(send_mesg, comm; dest=dst, tag=rank+32)\n",
        "\n",
        "stats = MPI.Waitall([rreq, sreq])\n",
        "\n",
        "print(\"$rank: Received $src -> $rank = $recv_mesg\\n\")\n",
        "\n",
        "MPI.Barrier(comm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPSOiIyiU0VH",
        "outputId": "e3bb8490-ec73-4708-fd97-290756eae95c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing 04-sendrecv.jl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now its execution:"
      ],
      "metadata": {
        "id": "a806djlqU67v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!/root/.julia/bin/mpiexecjl --project=/content -n 4 julia 04-sendrecv.jl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_oH6g_6oU-j3",
        "outputId": "31b138e9-2555-438f-b8ae-3d4838f33e85"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1: Sending   1 -> 2 = [1.0, 1.0, 1.0, 1.0]\n",
            "3: Sending   3 -> 0 = [3.0, 3.0, 3.0, 3.0]\n",
            "2: Sending   2 -> 3 = [2.0, 2.0, 2.0, 2.0]\n",
            "0: Sending   0 -> 1 = [0.0, 0.0, 0.0, 0.0]\n",
            "1: Received 0 -> 1 = [0.0, 0.0, 0.0, 0.0]\n",
            "3: Received 2 -> 3 = [2.0, 2.0, 2.0, 2.0]\n",
            "2: Received 1 -> 2 = [1.0, 1.0, 1.0, 1.0]\n",
            "0: Received 3 -> 0 = [3.0, 3.0, 3.0, 3.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "An example of how to use scatter and gather"
      ],
      "metadata": {
        "id": "BbDYLsSFVSHR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile 06-scatterv.jl\n",
        "# This example shows how to use MPI.Scatterv! and MPI.Gatherv!\n",
        "# roughly based on the example from\n",
        "# https://stackoverflow.com/a/36082684/392585\n",
        "\n",
        "using MPI\n",
        "\n",
        "\"\"\"\n",
        "    split_count(N::Integer, n::Integer)\n",
        "\n",
        "Return a vector of `n` integers which are approximately equally sized and sum to `N`.\n",
        "\"\"\"\n",
        "function split_count(N::Integer, n::Integer)\n",
        "    q,r = divrem(N, n)\n",
        "    return [i <= r ? q+1 : q for i = 1:n]\n",
        "end\n",
        "\n",
        "\n",
        "MPI.Init()\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = MPI.Comm_rank(comm)\n",
        "comm_size = MPI.Comm_size(comm)\n",
        "\n",
        "root = 0\n",
        "\n",
        "if rank == root\n",
        "    M, N = 4, 7\n",
        "\n",
        "    test = Float64[i for i = 1:M, j = 1:N]\n",
        "    output = similar(test)\n",
        "\n",
        "    # Julia arrays are stored in column-major order, so we need to split along the last dimension\n",
        "    # dimension\n",
        "    M_counts = [M for i = 1:comm_size]\n",
        "    N_counts = split_count(N, comm_size)\n",
        "\n",
        "    # store sizes in 2 * comm_size Array\n",
        "    sizes = vcat(M_counts', N_counts')\n",
        "    size_ubuf = UBuffer(sizes, 2)\n",
        "\n",
        "    # store number of values to send to each rank in comm_size length Vector\n",
        "    counts = vec(prod(sizes, dims=1))\n",
        "\n",
        "    test_vbuf = VBuffer(test, counts) # VBuffer for scatter\n",
        "    output_vbuf = VBuffer(output, counts) # VBuffer for gather\n",
        "else\n",
        "    # these variables can be set to `nothing` on non-root processes\n",
        "    size_ubuf = UBuffer(nothing)\n",
        "    output_vbuf = test_vbuf = VBuffer(nothing)\n",
        "end\n",
        "\n",
        "if rank == root\n",
        "    println(\"Original matrix\")\n",
        "    println(\"================\")\n",
        "    @show test sizes counts\n",
        "    println()\n",
        "    println(\"Each rank\")\n",
        "    println(\"================\")\n",
        "end\n",
        "MPI.Barrier(comm)\n",
        "\n",
        "local_size = MPI.Scatter(size_ubuf, NTuple{2,Int}, root, comm)\n",
        "local_test = MPI.Scatterv!(test_vbuf, zeros(Float64, local_size), root, comm)\n",
        "\n",
        "for i = 0:comm_size-1\n",
        "    if rank == i\n",
        "        @show rank local_test\n",
        "    end\n",
        "    MPI.Barrier(comm)\n",
        "end\n",
        "\n",
        "MPI.Gatherv!(local_test, output_vbuf, root, comm)\n",
        "\n",
        "if rank == root\n",
        "    println()\n",
        "    println(\"Final matrix\")\n",
        "    println(\"================\")\n",
        "    @show output\n",
        "end"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfMLckJfVWmH",
        "outputId": "7549fee0-32c6-4719-c5ef-98df1c65ee56"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing 06-scatterv.jl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!/root/.julia/bin/mpiexecjl --project=/content -n 4 julia 06-scatterv.jl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoudMhtUVd2v",
        "outputId": "f1752ac2-58c6-4757-f5ef-620b04f72914"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original matrix\n",
            "================\n",
            "test = [1.0 1.0 1.0 1.0 1.0 1.0 1.0; 2.0 2.0 2.0 2.0 2.0 2.0 2.0; 3.0 3.0 3.0 3.0 3.0 3.0 3.0; 4.0 4.0 4.0 4.0 4.0 4.0 4.0]\n",
            "sizes = [4 4 4 4; 2 2 2 1]\n",
            "counts = [8, 8, 8, 4]\n",
            "\n",
            "Each rank\n",
            "================\n",
            "rank = 0\n",
            "local_test = [1.0 1.0; 2.0 2.0; 3.0 3.0; 4.0 4.0]\n",
            "rank = 1\n",
            "local_test = [1.0 1.0; 2.0 2.0; 3.0 3.0; 4.0 4.0]\n",
            "rank = 2\n",
            "local_test = [1.0 1.0; 2.0 2.0; 3.0 3.0; 4.0 4.0]\n",
            "rank = 3\n",
            "local_test = [1.0; 2.0; 3.0; 4.0;;]\n",
            "\n",
            "Final matrix\n",
            "================\n",
            "output = [1.0 1.0 1.0 1.0 1.0 1.0 1.0; 2.0 2.0 2.0 2.0 2.0 2.0 2.0; 3.0 3.0 3.0 3.0 3.0 3.0 3.0; 4.0 4.0 4.0 4.0 4.0 4.0 4.0]\n"
          ]
        }
      ]
    }
  ]
}