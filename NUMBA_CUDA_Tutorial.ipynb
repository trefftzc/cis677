{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNySHCIg4rZny5n4h+a7mH7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trefftzc/cis677/blob/main/NUMBA_CUDA_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A NUMBA/CUDA Tutorial\n",
        "\n",
        "Based on https://numba.readthedocs.io/en/stable/cuda/kernels.html\n",
        "\n",
        "The execution environment needs some special setups:"
      ],
      "metadata": {
        "id": "yuji3cigGOZW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ZrpLznBSFHD3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['NUMBAPRO_LIBDEVICE'] = \"/usr/local/cuda-12.5/nvvm/libdevice\"\n",
        "os.environ['NUMBAPRO_NVVM'] = \"/usr/local/cuda-12.5/nvvm/lib64/libnvvm.so\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!uv pip install -q --system numba-cuda==0.4.0"
      ],
      "metadata": {
        "id": "UjsZo1h6F4JR"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "CUDA has an execution model unlike the traditional sequential model used for programming CPUs. In CUDA, the code you write will be executed by multiple threads at once (often hundreds or thousands). Your solution will be modeled by defining a thread hierarchy of grid, blocks and threads.\n",
        "\n",
        "Numba’s CUDA support exposes facilities to declare and manage this hierarchy of threads. The facilities are largely similar to those exposed by NVidia’s CUDA C language.\n",
        "\n",
        "Numba also exposes three kinds of GPU memory: global device memory (the large, relatively slow off-chip memory that’s connected to the GPU itself), on-chip shared memory and local memory. For all but the simplest algorithms, it is important that you carefully consider how to use and access memory in order to minimize bandwidth requirements and contention.\n",
        "\n",
        "# Kernel declaration\n",
        "\n",
        "A kernel function is a GPU function that is meant to be called from CPU code (*). It gives it two fundamental characteristics:\n",
        "\n",
        "kernels cannot explicitly return a value; all result data must be written to an array passed to the function (if computing a scalar, you will probably pass a one-element array);\n",
        "kernels explicitly declare their thread hierarchy when called: i.e. the number of thread blocks and the number of threads per block (note that while a kernel is compiled once, it can be called multiple times with different block sizes or grid sizes).\n",
        "At first sight, writing a CUDA kernel with Numba looks very much like writing a JIT function for the CPU:"
      ],
      "metadata": {
        "id": "EhwpuJ2GF-ql"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numba import config\n",
        "config.CUDA_ENABLE_PYNVJITLINK = 1"
      ],
      "metadata": {
        "id": "P3kPTo6YHlS9"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from numba import cuda\n",
        "@cuda.jit\n",
        "def increment_by_one(an_array):\n",
        "    \"\"\"\n",
        "    Increment all array elements by one.\n",
        "    \"\"\"\n",
        "    # code elided here; read further for different implementations"
      ],
      "metadata": {
        "id": "0F0td6lWGe7D"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kernel invocation\n",
        "\n",
        "A kernel is typically launched in the following way:"
      ],
      "metadata": {
        "id": "zO2nhmGaG4Wn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "threadsperblock = 32\n",
        "an_array = np.arange(2048)\n",
        "blockspergrid = (an_array.size + (threadsperblock - 1)) // threadsperblock\n",
        "increment_by_one[blockspergrid, threadsperblock](an_array)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMEdxgifG89Y",
        "outputId": "a6c3e568-e3a6-4ce5-c261-86dcc2147027"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/numba_cuda/numba/cuda/dispatcher.py:680: NumbaPerformanceWarning: Grid size 64 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.12/dist-packages/numba_cuda/numba/cuda/cudadrv/devicearray.py:940: NumbaPerformanceWarning: Host array used in CUDA kernel will incur copy overhead to/from device.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We notice two steps here:\n",
        "\n",
        "Instantiate the kernel proper, by specifying a number of blocks (or “blocks per grid”), and a number of threads per block. The product of the two will give the total number of threads launched. Kernel instantiation is done by taking the compiled kernel function (here increment_by_one) and indexing it with a tuple of integers.\n",
        "Running the kernel, by passing it the input array (and any separate output arrays if necessary). Kernels run asynchronously: launches queue their execution on the device and then return immediately. You can use cuda.synchronize() to wait for all previous kernel launches to finish executing.\n",
        "\n",
        "Note\n",
        "\n",
        "Passing an array that resides in host memory will implicitly cause a copy back to the host, which will be synchronous. In this case, the kernel launch will not return until the data is copied back, and therefore appears to execute synchronously."
      ],
      "metadata": {
        "id": "yYDjhCHwRIUX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Choosing the block size\n",
        "\n",
        "It might seem curious to have a two-level hierarchy when declaring the number of threads needed by a kernel. The block size (i.e. number of threads per block) is often crucial:\n",
        "\n",
        "On the software side, the block size determines how many threads share a given area of shared memory.\n",
        "On the hardware side, the block size must be large enough for full occupation of execution units; recommendations can be found in the CUDA C Programming Guide.\n",
        "\n",
        "# Multi-dimensional blocks and grids\n",
        "\n",
        "To help deal with multi-dimensional arrays, CUDA allows you to specify multi-dimensional blocks and grids. In the example above, you could make blockspergrid and threadsperblock tuples of one, two or three integers. Compared to 1D declarations of equivalent sizes, this doesn’t change anything to the efficiency or behaviour of generated code, but can help you write your algorithms in a more natural way.\n",
        "\n",
        "# Thread positioning\n",
        "\n",
        "When running a kernel, the kernel function’s code is executed by every thread once. It therefore has to know which thread it is in, in order to know which array element(s) it is responsible for (complex algorithms may define more complex responsibilities, but the underlying principle is the same).\n",
        "\n",
        "One way is for the thread to determine its position in the grid and block and manually compute the corresponding array position:"
      ],
      "metadata": {
        "id": "S76G0q0XRSTZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numba import cuda\n",
        "@cuda.jit\n",
        "def increment_by_one(an_array):\n",
        "    # Thread id in a 1D block\n",
        "    tx = cuda.threadIdx.x\n",
        "    # Block id in a 1D grid\n",
        "    ty = cuda.blockIdx.x\n",
        "    # Block width, i.e. number of threads per block\n",
        "    bw = cuda.blockDim.x\n",
        "    # Compute flattened index inside the array\n",
        "    pos = tx + ty * bw\n",
        "    if pos < an_array.size:  # Check array boundaries\n",
        "        an_array[pos] += 1"
      ],
      "metadata": {
        "id": "xKDnhhvORg9K"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unless you are sure the block size and grid size is a divisor of your array size, you must check boundaries as shown above.\n",
        "threadIdx, blockIdx, blockDim and gridDim are special objects provided by the CUDA backend for the sole purpose of knowing the geometry of the thread hierarchy and the position of the current thread within that geometry.\n",
        "\n",
        "These objects can be 1D, 2D or 3D, depending on how the kernel was invoked. To access the value at each dimension, use the x, y and z attributes of these objects, respectively.\n",
        "\n",
        "numba.cuda.threadIdx\n",
        "\n",
        "The thread indices in the current thread block. For 1D blocks, the index (given by the x attribute) is an integer spanning the range from 0 inclusive to numba.cuda.blockDim exclusive. A similar rule exists for each dimension when more than one dimension is used.\n",
        "\n",
        "numba.cuda.blockDim\n",
        "\n",
        "The shape of the block of threads, as declared when instantiating the kernel. This value is the same for all threads in a given kernel, even if they belong to different blocks (i.e. each block is “full”).\n",
        "numba.cuda.blockIdx\n",
        "The block indices in the grid of threads launched a kernel. For a 1D grid, the index (given by the x attribute) is an integer spanning the range from 0 inclusive to numba.cuda.gridDim exclusive. A similar rule exists for each dimension when more than one dimension is used.\n",
        "\n",
        "numba.cuda.gridDim\n",
        "\n",
        "The shape of the grid of blocks, i.e. the total number of blocks launched by this kernel invocation, as declared when instantiating the kernel.\n",
        "\n",
        "# Absolute positions\n",
        "\n",
        "Simple algorithms will tend to always use thread indices in the same way as shown in the example above. Numba provides additional facilities to automate such calculations:\n",
        "\n",
        "numba.cuda.grid(ndim)\n",
        "Return the absolute position of the current thread in the entire grid of blocks. ndim should correspond to the number of dimensions declared when instantiating the kernel. If ndim is 1, a single integer is returned. If ndim is 2 or 3, a tuple of the given number of integers is returned.\n",
        "numba.cuda.gridsize(ndim)\n",
        "Return the absolute size (or shape) in threads of the entire grid of blocks. ndim has the same meaning as in grid() above.\n",
        "\n",
        "With these functions, the incrementation example can become:"
      ],
      "metadata": {
        "id": "1Zl7CFsVRyVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@cuda.jit\n",
        "def increment_by_one(an_array):\n",
        "    pos = cuda.grid(1)\n",
        "    if pos < an_array.size:\n",
        "        an_array[pos] += 1"
      ],
      "metadata": {
        "id": "7WzlD-sqR_6l"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The same example for a 2D array and grid of threads would be:"
      ],
      "metadata": {
        "id": "2ap5a4kcSEBG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@cuda.jit\n",
        "def increment_a_2D_array(an_array):\n",
        "    x, y = cuda.grid(2)\n",
        "    if x < an_array.shape[0] and y < an_array.shape[1]:\n",
        "       an_array[x, y] += 1"
      ],
      "metadata": {
        "id": "aV6q6meoSEzd"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note the grid computation when instantiating the kernel must still be done manually, for example:"
      ],
      "metadata": {
        "id": "QltP4cZCSK-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "an_array = np.arange(1024).reshape(32,32)\n",
        "threadsperblock = (16, 16)\n",
        "blockspergrid_x = math.ceil(an_array.shape[0] / threadsperblock[0])\n",
        "blockspergrid_y = math.ceil(an_array.shape[1] / threadsperblock[1])\n",
        "blockspergrid = (blockspergrid_x, blockspergrid_y)\n",
        "increment_a_2D_array[blockspergrid, threadsperblock](an_array)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJRyDY4OSNR-",
        "outputId": "cc036a0a-d084-4458-db76-b8a0cd0e403b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/numba_cuda/numba/cuda/dispatcher.py:680: NumbaPerformanceWarning: Grid size 4 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.12/dist-packages/numba_cuda/numba/cuda/cudadrv/devicearray.py:940: NumbaPerformanceWarning: Host array used in CUDA kernel will incur copy overhead to/from device.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Memory management\n",
        "\n",
        "# Data transfer\n",
        "\n",
        "Even though Numba can automatically transfer NumPy arrays to the device, it can only do so conservatively by always transferring device memory back to the host when a kernel finishes. To avoid the unnecessary transfer for read-only arrays, you can use the following APIs to manually control the transfer:\n",
        "\n",
        "numba.cuda.device_array(shape, dtype=np.float64, strides=None, order='C', stream=0)\n",
        "\n",
        "Allocate an empty device ndarray. Similar to numpy.empty().\n",
        "\n",
        "numba.cuda.device_array_like(ary, stream=0)\n",
        "\n",
        "Call device_array() with information from the array.\n",
        "\n",
        "numba.cuda.to_device(obj, stream=0, copy=True, to=None)\n",
        "\n",
        "Allocate and transfer a numpy ndarray or structured scalar to the device.\n",
        "\n",
        "To copy host->device a numpy array:"
      ],
      "metadata": {
        "id": "J3a4GIgVTfo4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ary = np.arange(10)\n",
        "d_ary = cuda.to_device(ary)"
      ],
      "metadata": {
        "id": "cDuRoYxPUGEU"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The resulting d_ary is a DeviceNDArray.\n",
        "\n",
        "To copy device->host:"
      ],
      "metadata": {
        "id": "Am8mhyQsUNau"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hary = d_ary.copy_to_host()"
      ],
      "metadata": {
        "id": "VDdrHaMPUP2O"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To copy device->host to an existing array:"
      ],
      "metadata": {
        "id": "VU0rTdcfUTU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ary = np.empty(shape=d_ary.shape, dtype=d_ary.dtype)\n",
        "d_ary.copy_to_host(ary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzG41mEQUVQn",
        "outputId": "a255dd34-9683-4245-af60-d4aed5985c20"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Device arrays\n",
        "\n",
        "Device array references have the following methods. These methods are to be called in host code, not within CUDA-jitted functions.\n",
        "\n",
        "class numba.cuda.cudadrv.devicearray.DeviceNDArray(shape, strides, dtype, stream=0, gpu_data=None)\n",
        "\n",
        "An on-GPU array type\n",
        "\n",
        "copy_to_host(ary=None, stream=0)\n",
        "\n",
        "Copy self to ary or create a new Numpy ndarray if ary is None.\n",
        "\n",
        "Always returns the host array.\n",
        "\n",
        "Example:"
      ],
      "metadata": {
        "id": "WyKJ_y_SUhMw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numba import cuda\n",
        "\n",
        "@cuda.jit\n",
        "def my_kernel(an_array):\n",
        "    x = cuda.grid(1)\n",
        "    if x < an_array.shape[0]:\n",
        "       an_array[x] += 1\n",
        "\n",
        "arr = np.arange(10000)\n",
        "d_arr = cuda.to_device(arr)\n",
        "\n",
        "my_kernel[100, 100](d_arr)\n",
        "\n",
        "result_array = d_arr.copy_to_host()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBp46QdZUs6y",
        "outputId": "eae4f974-396d-4230-9818-a46bed148bea"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/numba_cuda/numba/cuda/dispatcher.py:680: NumbaPerformanceWarning: Grid size 100 will likely result in GPU under-utilization due to low occupancy.\n",
            "  and not config.DISABLE_PERFORMANCE_WARNINGS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Examples\n",
        "\n",
        "\n",
        "# Vector Addition\n",
        "\n",
        "This example uses Numba to create on-device arrays and a vector addition kernel; it is a warmup for learning how to write GPU kernels using Numba. We’ll begin with some required imports:\n",
        "\n"
      ],
      "metadata": {
        "id": "sIUbixIGV_As"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numba import cuda"
      ],
      "metadata": {
        "id": "Jc8poXIWWMZi"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following function is the kernel. Note that it is defined in terms of Python variables with unspecified types. When the kernel is launched, Numba will examine the types of the arguments that are passed at runtime and generate a CUDA kernel specialized for them.\n",
        "\n",
        "Note that Numba kernels do not return values and must write any output into arrays passed in as parameters (this is similar to the requirement that CUDA C/C++ kernels have void return type). Here we pass in c for the results to be written into."
      ],
      "metadata": {
        "id": "fc7u4b3eWPmG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@cuda.jit\n",
        "def f(a, b, c):\n",
        "    # like threadIdx.x + (blockIdx.x * blockDim.x)\n",
        "    tid = cuda.grid(1)\n",
        "    size = len(c)\n",
        "\n",
        "    if tid < size:\n",
        "        c[tid] = a[tid] + b[tid]"
      ],
      "metadata": {
        "id": "N3T7lAWWWR9H"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "cuda.to_device() can be used create device-side copies of arrays. cuda.device_array_like() creates an uninitialized array of the same shape and type as an existing array. Here we transfer two vectors and create an empty vector to hold our results:"
      ],
      "metadata": {
        "id": "AwZDEZ8pWbQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N = 100000\n",
        "a = cuda.to_device(np.random.random(N))\n",
        "b = cuda.to_device(np.random.random(N))\n",
        "c = cuda.device_array_like(a)"
      ],
      "metadata": {
        "id": "FAnuJOO_WcFo"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A call to forall() generates an appropriate launch configuration with a 1D grid (see Kernel invocation) for a given data size and is often the simplest way of launching a kernel:"
      ],
      "metadata": {
        "id": "xnLu0eeIWje5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f.forall(len(a))(a, b, c)\n",
        "print(c.copy_to_host())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COrVnG2iWkjp",
        "outputId": "f602f84f-fd19-4575-e5c4-f302eeb400d2"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.17708359 1.40638864 0.52871927 ... 0.98011679 0.42299897 1.64504293]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/numba_cuda/numba/cuda/dispatcher.py:680: NumbaPerformanceWarning: Grid size 98 will likely result in GPU under-utilization due to low occupancy.\n",
            "  and not config.DISABLE_PERFORMANCE_WARNINGS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "One can also configure the grid manually using the subscripting syntax. The following example launches a grid with sufficient threads to operate on every vector element:"
      ],
      "metadata": {
        "id": "dB3ksmBHW1xd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enough threads per block for several warps per block\n",
        "nthreads = 256\n",
        "# Enough blocks to cover the entire vector depending on its length\n",
        "nblocks = (len(a) // nthreads) + 1\n",
        "f[nblocks, nthreads](a, b, c)\n",
        "print(c.copy_to_host())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dw8tEFLUW6UD",
        "outputId": "f27103d5-4a2f-46f1-d3e1-47c6a123df4e"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.17708359 1.40638864 0.52871927 ... 0.98011679 0.42299897 1.64504293]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1D Heat Equation\n",
        "\n",
        "This example solves Laplace’s equation in one dimension for a certain set of initial conditions and boundary conditions. A full discussion of Laplace’s equation is out of scope for this documentation, but it will suffice to say that it describes how heat propagates through an object over time. It works by discretizing the problem in two ways:\n",
        "\n",
        "The domain is partitioned into a mesh of points that each have an individual temperature.\n",
        "Time is partitioned into discrete intervals that are advanced forward sequentially.\n",
        "Then, the following assumption is applied: The temperature of a point after some interval has passed is some weighted average of the temperature of the points that are directly adjacent to it. Intuitively, if all the points in the domain are very hot and a single point in the middle is very cold, as time passes, the hot points will cause the cold one to heat up and the cold point will cause the surrounding hot pieces to cool slightly. Simply put, the heat spreads throughout the object.\n",
        "\n",
        "We can implement this simulation using a Numba kernel. Let’s start simple by assuming we have a one dimensional object which we’ll represent with an array of values. The position of the element in the array is the position of a point within the object, and the value of the element represents the temperature."
      ],
      "metadata": {
        "id": "6yy2pEsGXJ6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numba import cuda"
      ],
      "metadata": {
        "id": "bdX50lZlXOlo"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use an odd problem size.\n",
        " # This is so there can be an element truly in the \"middle\" for symmetry.\n",
        "size = 1001\n",
        "data = np.zeros(size)\n",
        "\n",
        " # Middle element is made very hot\n",
        "data[500] = 10000\n",
        "buf_0 = cuda.to_device(data)\n",
        "\n",
        "# This extra array is used for synchronization purposes\n",
        "buf_1 = cuda.device_array_like(buf_0)\n",
        "\n",
        "niter = 10000"
      ],
      "metadata": {
        "id": "rzSQnc8iXUYx"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our kernel each thread will be responsible for managing the temperature update for a single element in a loop over the desired number of timesteps. The kernel is below. Note the use of cooperative group synchronization and the use of two buffers swapped at each iteration to avoid race conditions. See numba.cuda.cg.this_grid() for details."
      ],
      "metadata": {
        "id": "DHGB2oTBXcYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@cuda.jit\n",
        "def solve_heat_equation(buf_0, buf_1, timesteps, k):\n",
        "     i = cuda.grid(1)\n",
        "\n",
        "     # Don't continue if our index is outside the domain\n",
        "     if i >= len(buf_0):\n",
        "         return\n",
        "\n",
        "     # Prepare to do a grid-wide synchronization later\n",
        "     grid = cuda.cg.this_grid()\n",
        "\n",
        "     for step in range(timesteps):\n",
        "        # Select the buffer from the previous timestep\n",
        "        if (step % 2) == 0:\n",
        "            data = buf_0\n",
        "            next_data = buf_1\n",
        "        else:\n",
        "            data = buf_1\n",
        "            next_data = buf_0\n",
        "\n",
        "        # Get the current temperature associated with this point\n",
        "        curr_temp = data[i]\n",
        "\n",
        "        # Apply formula from finite difference equation\n",
        "        if i == 0:\n",
        "            # Left wall is held at T = 0\n",
        "            next_temp = curr_temp + k * (data[i + 1] - (2 * curr_temp))\n",
        "        elif i == len(data) - 1:\n",
        "            # Right wall is held at T = 0\n",
        "            next_temp = curr_temp + k * (data[i - 1] - (2 * curr_temp))\n",
        "        else:\n",
        "            # Interior points are a weighted average of their neighbors\n",
        "            next_temp = curr_temp + k * (\n",
        "                data[i - 1] - (2 * curr_temp) + data[i + 1]\n",
        "            )\n",
        "\n",
        "        # Write new value to the next buffer\n",
        "        next_data[i] = next_temp\n",
        "\n",
        "        # Wait for every thread to write before moving on\n",
        "        grid.sync()"
      ],
      "metadata": {
        "id": "BaxvJf6aXfao"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calling the kernel:"
      ],
      "metadata": {
        "id": "oWSq71EDX6qW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "solve_heat_equation.forall(len(data))(\n",
        "    buf_0, buf_1, niter, 0.25\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94lbWO8qX8Mk",
        "outputId": "063bb341-cc6a-4107-f596-157f16e5bd8d"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/numba_cuda/numba/cuda/dispatcher.py:680: NumbaPerformanceWarning: Grid size 1 will likely result in GPU under-utilization due to low occupancy.\n",
            "  and not config.DISABLE_PERFORMANCE_WARNINGS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting the final data shows an arc that is highest where the object was hot initially and gradually sloping down to zero towards the edges where the temperature is fixed at zero. In the limit of infinite time, the arc will flatten out completely."
      ],
      "metadata": {
        "id": "ywK3oZ78YOI9"
      }
    }
  ]
}